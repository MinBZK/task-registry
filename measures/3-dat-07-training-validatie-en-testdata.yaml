systemcard_path: .systemcard.requirements[]
schema_version: 1.1.0
name: Gebruik bij machine learning technieken gescheiden train-, test- en validatiedata
  en houdt rekening met underfitting en overfitting.
description: "Indien je gebruik maakt van machine learning technieken, maak een passende\
  \ keuze voor gescheiden train-, test- en validatiedata en houd hierbij rekening\
  \ met underfitting en overfitting. \n"
explanation: "Verdeel je dataset in drie delen:\n\n1. **de trainingsset**\n\n    Deze\
  \ dataset wordt gebruikt om het model te trainen. Uit deze dataset worden de onderliggende\
  \ patronen of relaties geleerd die later gebruikt kunnen worden om voorspellingen\
  \ mee te doen.\n\n    De [kwaliteit](3-dat-01-datakwaliteit.md) van deze dataset\
  \ moet goed zijn en zo representatief mogelijk voor de doelpopulatie. Eventuele\
  \ [bias](../../onderwerpen/bias-en-non-discriminatie.md#verschillende-vormen-van-bias)\
  \ of vooroordelen in deze dataset kan door het trainen in het model sluipen.\n\n\
  \   Let bij het samenstellen van de traningsset op dat de data waarop het model\
  \ gebaseerd is, niet beschikbaar zijn voordat de uitkomsten zijn geobserveerd. Met\
  \ andere woorden, zorg ervoor de de voorspellingen geen onderdeel kunnen zijn van\
  \ de inputvariabelen. \n\n3. **de validatieset**\n\n    De validatieset fungeert\
  \ als een onafhankelijke, onbevooroordeelde dataset voor het vergelijken van de\
  \ prestaties van verschillende algoritmes die zijn getraind op onze trainingsset.\n\
  \n    Verschillende modellen kunnen getraind worden op de trainingsdataset. Zo kan\
  \ je bijvoorbeeld variÃ«ren in de (hyper)parameters of de inputvariabelen. Dit leidt\
  \ tot verschillende varianten van het model. Om de prestaties van de verschillende\
  \ modellen te vergelijken, moeten we een nieuwe dataset gebruiken: de validatieset.\
  \ Zou je hiervoor de trainingsdataset gebruiken, kan dat leiden tot [overfitting](https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html),\
  \ wanneer het model dan te specifiek afgestemd is op 1 dataset. Het model kan dan\
  \ niet voldoende generaliseren voor nieuwe situaties.\n\n4. **de testset**\n\n \
  \   Nadat er met behulp van de validatieset een keuze is gemaakt voor een passend\
  \ model en bijbehorende (hyper)parameters, moet je het model nog testen op nieuwe\
  \ data. Dit geeft een beeld van de werkelijke prestaties van het model in nieuwe\
  \ omstandigheden. \n\n    Let op dat je pas naar deze resultaten kijkt als laatste\
  \ stap. Inzichten uit deze testdataset mogen niet worden meegenomen in de ontwikkeling,\
  \ omdat dit kan leiden tot overfitting. Het model zal dan in productie mogelijk\
  \ minder goed presteren. \n\n### Grootte van de drie datasets\n\nEr is geen optimale\
  \ verdeling van de drie datsets. Veelvoorkomende verhoudingen om je data in te splitten\
  \ zijn:\n\n- 80% trainingsset, 10% validatieset, 10% testset\n- 70% trainingsset,\
  \ 15% validatieset, 15% testset\n- 60% trainingsset, 20% validatieset, 20% testset\n\
  \nAfhankelijk van de hoeveelheid beschikbare data en de context maak je hierin een\
  \ keuze. Houdt hierbij rekening met:\n\n- Hoe minder trainingdata, hoe groter de\
  \ variantie van het model tijdens het trainen. De patronen en relaties die ontdekt\
  \ zijn, bevatten dan een grotere onzekerheid. \n- Hoe minder validatie- en testdata\
  \ je gebruikt, hoe grote de variantie en de onzekerheid in de verwachte prestaties\
  \ van het algoritme. \n- Hoe complexer het model, en hoe meer (hyper)parameters\
  \ er zijn om te optimaliserne, hoe groter de validatieset moet zijn om het model\
  \ met optimale presetaties te vinden. Wanneer er weinig hyperparameters zijn, is\
  \ een relatief kleine validatieset vaak voldoende.\n\n### k-fold cross validation\n\
  \nNaast dat je de datasets willekeurig kan verdelen in drie delen (aselect), kan\
  \ je ook meer geavanceerde technieken gebruiken. Een robuuste en veelgebruikte techniek\
  \ is [k-fold cross validation](https://hastie.su.domains/ISLP/ISLP_website.pdf.download.html),\
  \ waarbij het model *k* keer wordt getraind op verschillende delen van de data.\
  \ \n\nDoor onjuiste training van het model presteert het model in de praktijk minder\
  \ goed dan bij de tests. Als trainings-, validatie- en testdata door elkaar lopen\
  \ (\"data leakage\"), kan dit leiden tot overfitting, waardoor het model beter lijkt\
  \ te presteren dan in werkelijkheid het geval is.\n"
urn: urn:nl:ak:mtr:dat-07
language: nl
owners:
- organization: Algoritmekader
  name: ''
  email: ''
  role: ''
date: ''
url: https://minbzk.github.io/Algoritmekader/voldoen-aan-wetten-en-regels/maatregelen/3-dat-07-training-validatie-en-testdata/index.html
subject:
- data
- technische-robuustheid-en-veiligheid
- bias-en-non-discriminatie
suggested_roles:
- ontwikkelaar
lifecycle:
- dataverkenning-en-datapreparatie
- ontwikkelen
links:
- urn:nl:ak:ver:aia-10
template:
  requirement: $REQUIREMENT
  remarks: $REMARKS
  status: $STATUS
  timestamp: $TIMESTAMP
  authors:
  - name: $AUTHOR.NAME
    email: $AUTHOR.EMAIL
    role: $AUTHOR.ROLE
